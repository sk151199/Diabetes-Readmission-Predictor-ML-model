{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d575b71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a45b9e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetic_data = pd.read_csv(\"diabetic_data.csv\")\n",
    "ids_mapping = pd.read_csv(\"IDS_mapping.csv\")\n",
    "diabetes_features = pd.read_csv(\"diabetes_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8bb130b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "max_glu_serum               96420\n",
       "A1Cresult                   84748\n",
       "race                            0\n",
       "gender                          0\n",
       "age                             0\n",
       "weight                          0\n",
       "admission_type_id               0\n",
       "discharge_disposition_id        0\n",
       "admission_source_id             0\n",
       "time_in_hospital                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show first few rows\n",
    "diabetic_data.head()\n",
    "\n",
    "# Show column names\n",
    "diabetic_data.columns\n",
    "\n",
    "# Check missing values\n",
    "diabetic_data.isna().sum().sort_values(ascending=False).head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6b367320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diabetic Data columns:\n",
      "['encounter_id', 'patient_nbr', 'race', 'gender', 'age', 'weight', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id', 'time_in_hospital', 'payer_code', 'medical_specialty', 'num_lab_procedures', 'num_procedures', 'num_medications', 'number_outpatient', 'number_emergency', 'number_inpatient', 'diag_1', 'diag_2', 'diag_3', 'number_diagnoses', 'max_glu_serum', 'A1Cresult', 'metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride', 'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide', 'pioglitazone', 'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone', 'tolazamide', 'examide', 'citoglipton', 'insulin', 'glyburide-metformin', 'glipizide-metformin', 'glimepiride-pioglitazone', 'metformin-rosiglitazone', 'metformin-pioglitazone', 'change', 'diabetesMed', 'readmitted']\n",
      "\n",
      "Diabetes Features columns:\n",
      "['Feature name', 'Type', 'Description and values']\n",
      "\n",
      "IDS Mapping columns:\n",
      "['admission_type_id', 'description']\n"
     ]
    }
   ],
   "source": [
    "print(\"Diabetic Data columns:\")\n",
    "print(diabetic_data.columns.tolist())\n",
    "\n",
    "print(\"\\nDiabetes Features columns:\")\n",
    "print(diabetes_features.columns.tolist())\n",
    "\n",
    "print(\"\\nIDS Mapping columns:\")\n",
    "print(ids_mapping.columns.tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c74c743d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admission_type_id</th>\n",
       "      <th>admission_type_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>Discharged/transferred to home with home healt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>Transfer from another health care facility</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Emergency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Discharged to home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>Physician Referral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>Emergency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>Discharged to home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>Physician Referral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>Emergency</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  admission_type_id                         admission_type_description\n",
       "0                 6                                                NaN\n",
       "1                 6  Discharged/transferred to home with home healt...\n",
       "2                 6         Transfer from another health care facility\n",
       "3                 1                                          Emergency\n",
       "4                 1                                 Discharged to home\n",
       "5                 1                                 Physician Referral\n",
       "6                 1                                          Emergency\n",
       "7                 1                                 Discharged to home\n",
       "8                 1                                 Physician Referral\n",
       "9                 1                                          Emergency"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert both columns to string type\n",
    "diabetic_data['admission_type_id'] = diabetic_data['admission_type_id'].astype(str)\n",
    "ids_mapping['admission_type_id'] = ids_mapping['admission_type_id'].astype(str)\n",
    "\n",
    "# Now merge safely\n",
    "merged_df = diabetic_data.merge(ids_mapping, on=\"admission_type_id\", how=\"left\")\n",
    "\n",
    "# Rename column for clarity\n",
    "merged_df.rename(columns={'description': 'admission_type_description'}, inplace=True)\n",
    "\n",
    "# Check result\n",
    "merged_df[['admission_type_id', 'admission_type_description']].head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ce8d9900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['encounter_id',\n",
       " 'patient_nbr',\n",
       " 'race',\n",
       " 'gender',\n",
       " 'age',\n",
       " 'weight',\n",
       " 'admission_type_id',\n",
       " 'discharge_disposition_id',\n",
       " 'admission_source_id',\n",
       " 'time_in_hospital',\n",
       " 'payer_code',\n",
       " 'medical_specialty',\n",
       " 'num_lab_procedures',\n",
       " 'num_procedures',\n",
       " 'num_medications',\n",
       " 'number_outpatient',\n",
       " 'number_emergency',\n",
       " 'number_inpatient',\n",
       " 'diag_1',\n",
       " 'diag_2',\n",
       " 'diag_3',\n",
       " 'number_diagnoses',\n",
       " 'max_glu_serum',\n",
       " 'A1Cresult',\n",
       " 'metformin',\n",
       " 'repaglinide',\n",
       " 'nateglinide',\n",
       " 'chlorpropamide',\n",
       " 'glimepiride',\n",
       " 'acetohexamide',\n",
       " 'glipizide',\n",
       " 'glyburide',\n",
       " 'tolbutamide',\n",
       " 'pioglitazone',\n",
       " 'rosiglitazone',\n",
       " 'acarbose',\n",
       " 'miglitol',\n",
       " 'troglitazone',\n",
       " 'tolazamide',\n",
       " 'examide',\n",
       " 'citoglipton',\n",
       " 'insulin',\n",
       " 'glyburide-metformin',\n",
       " 'glipizide-metformin',\n",
       " 'glimepiride-pioglitazone',\n",
       " 'metformin-rosiglitazone',\n",
       " 'metformin-pioglitazone',\n",
       " 'change',\n",
       " 'diabetesMed',\n",
       " 'readmitted',\n",
       " 'admission_type_description']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.columns.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2faf378c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def encode_a1c_and_glu(merged_df, drop_original=True):\n",
    "    merged_df = merged_df.copy()\n",
    "\n",
    "    # Standardize placeholder missing tokens\n",
    "    repl = {\"?\": np.nan, \"None\": np.nan, \"Unknown\": np.nan, \"Unknown/Invalid\": np.nan, \"\": np.nan, \" \": np.nan}\n",
    "    for col in [\"A1Cresult\", \"max_glu_serum\"]:\n",
    "        if col in merged_df.columns:\n",
    "            merged_df[col] = merged_df[col].replace(repl)\n",
    "\n",
    "    # --- Encode A1C ---\n",
    "    if \"A1Cresult\" in merged_df.columns:\n",
    "        merged_df[\"a1c_measured\"] = (~merged_df[\"A1Cresult\"].isna()).astype(int)\n",
    "        merged_df[\"a1c_severity\"] = merged_df[\"A1Cresult\"].map({\n",
    "            \"Norm\": 0,\n",
    "            \">7\": 1,\n",
    "            \">8\": 2\n",
    "        })\n",
    "\n",
    "    # --- Encode max_glu_serum ---\n",
    "    if \"max_glu_serum\" in merged_df.columns:\n",
    "        merged_df[\"glu_measured\"] = (~merged_df[\"max_glu_serum\"].isna()).astype(int)\n",
    "        merged_df[\"glu_severity\"] = merged_df[\"max_glu_serum\"].map({\n",
    "            \"Norm\": 0,\n",
    "            \">200\": 1,\n",
    "            \">300\": 2\n",
    "        })\n",
    "\n",
    "    # --- Sanity check for unexpected values ---\n",
    "    for col, mapping in {\n",
    "        \"A1Cresult\": {\"Norm\": 0, \">7\": 1, \">8\": 2},\n",
    "        \"max_glu_serum\": {\"Norm\": 0, \">200\": 1, \">300\": 2}\n",
    "    }.items():\n",
    "        if col in merged_df.columns:\n",
    "            unmatched = sorted(set(merged_df[col].dropna()) - set(mapping.keys()))\n",
    "            if unmatched:\n",
    "                print(f\"[info] Unmapped values in {col}: {unmatched}\")\n",
    "\n",
    "    # Drop the original columns if we no longer need them\n",
    "    if drop_original:\n",
    "        merged_df.drop(columns=[c for c in [\"A1Cresult\", \"max_glu_serum\"] if c in merged_df.columns],\n",
    "                       inplace=True)\n",
    "\n",
    "    return merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5de054ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = encode_a1c_and_glu(merged_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "464a923a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a1c_measured</th>\n",
       "      <th>a1c_severity</th>\n",
       "      <th>glu_measured</th>\n",
       "      <th>glu_severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    a1c_measured  a1c_severity  glu_measured  glu_severity\n",
       "0              0           NaN             0           NaN\n",
       "1              0           NaN             0           NaN\n",
       "2              0           NaN             0           NaN\n",
       "3              0           NaN             0           NaN\n",
       "4              0           NaN             0           NaN\n",
       "..           ...           ...           ...           ...\n",
       "95             0           NaN             0           NaN\n",
       "96             0           NaN             0           NaN\n",
       "97             0           NaN             0           NaN\n",
       "98             0           NaN             0           NaN\n",
       "99             0           NaN             0           NaN\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df[[\"a1c_measured\", \"a1c_severity\", \"glu_measured\", \"glu_severity\"]].head(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5b627516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 0 columns (> 70% missing):\n",
      "[]\n",
      "\n",
      "✅ Columns dropped successfully.\n",
      "Remaining columns: 53\n",
      "New shape: (305298, 53)\n",
      "\n",
      "Top remaining missing %:\n",
      " glu_severity                  94.746772\n",
      "a1c_severity                  83.277322\n",
      "admission_type_description     1.733061\n",
      "patient_nbr                    0.000000\n",
      "encounter_id                   0.000000\n",
      "weight                         0.000000\n",
      "admission_type_id              0.000000\n",
      "discharge_disposition_id       0.000000\n",
      "admission_source_id            0.000000\n",
      "time_in_hospital               0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1️⃣ Compute missing percentage for each column\n",
    "missing_pct = (merged_df.isna().mean() * 100).sort_values(ascending=False)\n",
    "\n",
    "# 2️⃣ Define threshold and protected engineered columns\n",
    "THRESH = 70  # drop columns with more than 70% missing values\n",
    "protect_cols = {\"a1c_measured\", \"a1c_severity\", \"glu_measured\", \"glu_severity\"}\n",
    "\n",
    "# 3️⃣ Find columns to drop\n",
    "cols_to_drop = [\n",
    "    c for c in missing_pct.index\n",
    "    if missing_pct[c] > THRESH and c not in protect_cols\n",
    "]\n",
    "\n",
    "print(f\"Dropping {len(cols_to_drop)} columns (> {THRESH}% missing):\")\n",
    "print(cols_to_drop)\n",
    "\n",
    "# 4️⃣ Drop them\n",
    "merged_df.drop(columns=cols_to_drop, inplace=True, errors=\"ignore\")\n",
    "\n",
    "# 5️⃣ Display summary after dropping\n",
    "print(\"\\n✅ Columns dropped successfully.\")\n",
    "print(\"Remaining columns:\", len(merged_df.columns))\n",
    "print(\"New shape:\", merged_df.shape)\n",
    "\n",
    "# 6️⃣ Optional: show top 10 columns still having missing values\n",
    "print(\"\\nTop remaining missing %:\\n\",\n",
    "      (merged_df.isna().mean() * 100).sort_values(ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7b2bc4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 7 noisy / ID columns:\n",
      "['encounter_id', 'patient_nbr', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id', 'examide', 'citoglipton']\n",
      "\n",
      "✅ Noise columns removed successfully.\n",
      "Remaining columns: 46\n",
      "New shape: (305298, 46)\n"
     ]
    }
   ],
   "source": [
    "# 1️⃣ Drop pure identifiers and raw codes (keep descriptions instead)\n",
    "id_like = [\n",
    "    \"encounter_id\", \"patient_nbr\",\n",
    "    \"admission_type_id\", \"discharge_disposition_id\", \"admission_source_id\"\n",
    "]\n",
    "\n",
    "# 2️⃣ Drop constant or near-constant columns\n",
    "constant_cols = [c for c in merged_df.columns if merged_df[c].nunique(dropna=True) <= 1]\n",
    "\n",
    "# 3️⃣ Combine all into one list (keep A1C/GLU safe)\n",
    "protect_cols = {\"a1c_measured\", \"a1c_severity\", \"glu_measured\", \"glu_severity\"}\n",
    "noise_cols = [c for c in (id_like + constant_cols) if c not in protect_cols]\n",
    "\n",
    "print(f\"Dropping {len(noise_cols)} noisy / ID columns:\")\n",
    "print(noise_cols)\n",
    "\n",
    "merged_df.drop(columns=noise_cols, inplace=True, errors=\"ignore\")\n",
    "\n",
    "print(\"\\n✅ Noise columns removed successfully.\")\n",
    "print(\"Remaining columns:\", len(merged_df.columns))\n",
    "print(\"New shape:\", merged_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1ab1a764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 7 low-variance columns:\n",
      "['acetohexamide', 'tolbutamide', 'troglitazone', 'glipizide-metformin', 'glimepiride-pioglitazone', 'metformin-rosiglitazone', 'metformin-pioglitazone']\n",
      "✅ Low-variance columns removed. New shape: (305298, 39)\n"
     ]
    }
   ],
   "source": [
    "# Find very low variance columns (almost all same value)\n",
    "low_var_cols = [c for c in merged_df.columns\n",
    "                if merged_df[c].nunique() <= 2 and merged_df[c].value_counts(normalize=True).iloc[0] > 0.95]\n",
    "\n",
    "low_var_cols = [c for c in low_var_cols if c not in protect_cols]\n",
    "print(f\"Dropping {len(low_var_cols)} low-variance columns:\")\n",
    "print(low_var_cols)\n",
    "\n",
    "merged_df.drop(columns=low_var_cols, inplace=True, errors=\"ignore\")\n",
    "print(\"✅ Low-variance columns removed. New shape:\", merged_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e9935435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining columns: ['race', 'gender', 'age', 'weight', 'time_in_hospital', 'payer_code', 'medical_specialty', 'num_lab_procedures', 'num_procedures', 'num_medications', 'number_outpatient', 'number_emergency', 'number_inpatient', 'diag_1', 'diag_2', 'diag_3', 'number_diagnoses', 'metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride', 'glipizide', 'glyburide', 'pioglitazone', 'rosiglitazone', 'acarbose', 'miglitol', 'tolazamide', 'insulin', 'glyburide-metformin', 'change', 'diabetesMed', 'readmitted', 'admission_type_description', 'a1c_measured', 'a1c_severity', 'glu_measured', 'glu_severity']\n",
      "Final shape: (305298, 39)\n"
     ]
    }
   ],
   "source": [
    "print(\"Remaining columns:\", merged_df.columns.tolist())\n",
    "print(\"Final shape:\", merged_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "30e0665e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 0 highly correlated columns:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Compute correlation matrix\n",
    "corr_matrix = merged_df.select_dtypes(include=[np.number]).corr().abs()\n",
    "\n",
    "# Keep only upper triangle of matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Find columns with correlation > 0.85\n",
    "high_corr_cols = [column for column in upper.columns if any(upper[column] > 0.85)]\n",
    "\n",
    "print(f\"Dropping {len(high_corr_cols)} highly correlated columns:\")\n",
    "print(high_corr_cols)\n",
    "\n",
    "merged_df.drop(columns=high_corr_cols, inplace=True, errors=\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e0c18e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 0 near-constant columns:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "low_var_cols = [c for c in merged_df.columns\n",
    "                if merged_df[c].nunique() <= 2 and merged_df[c].value_counts(normalize=True).iloc[0] > 0.98]\n",
    "\n",
    "print(f\"Dropping {len(low_var_cols)} near-constant columns:\")\n",
    "print(low_var_cols)\n",
    "\n",
    "merged_df.drop(columns=low_var_cols, inplace=True, errors=\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "88aaaa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_noise = [\"payer_code\", \"medical_specialty\"]\n",
    "\n",
    "merged_df.drop(columns=[c for c in domain_noise if c in merged_df.columns],\n",
    "               inplace=True, errors=\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a989fad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['race',\n",
       " 'gender',\n",
       " 'age',\n",
       " 'weight',\n",
       " 'time_in_hospital',\n",
       " 'num_lab_procedures',\n",
       " 'num_procedures',\n",
       " 'num_medications',\n",
       " 'number_outpatient',\n",
       " 'number_emergency',\n",
       " 'number_inpatient',\n",
       " 'diag_1',\n",
       " 'diag_2',\n",
       " 'diag_3',\n",
       " 'number_diagnoses',\n",
       " 'metformin',\n",
       " 'repaglinide',\n",
       " 'nateglinide',\n",
       " 'chlorpropamide',\n",
       " 'glimepiride',\n",
       " 'glipizide',\n",
       " 'glyburide',\n",
       " 'pioglitazone',\n",
       " 'rosiglitazone',\n",
       " 'acarbose',\n",
       " 'miglitol',\n",
       " 'tolazamide',\n",
       " 'insulin',\n",
       " 'glyburide-metformin',\n",
       " 'change',\n",
       " 'diabetesMed',\n",
       " 'readmitted',\n",
       " 'admission_type_description',\n",
       " 'a1c_measured',\n",
       " 'a1c_severity',\n",
       " 'glu_measured',\n",
       " 'glu_severity']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.columns.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c80b0b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Reduced to key predictive features. Shape: (305298, 16)\n"
     ]
    }
   ],
   "source": [
    "key_features = [\n",
    "    \"race\", \"gender\", \"age_mid\", \"time_in_hospital\", \"num_medications\",\n",
    "    \"number_inpatient\", \"number_emergency\", \"number_outpatient\", \"number_diagnoses\",\n",
    "    \"a1c_measured\", \"a1c_severity\", \"glu_measured\", \"glu_severity\",\n",
    "    \"change\", \"diabetesMed\", \"insulin\"\n",
    "]\n",
    "\n",
    "merged_df = merged_df[[c for c in key_features if c in merged_df.columns] + [\"readmitted\"]]\n",
    "print(\"✅ Reduced to key predictive features. Shape:\", merged_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6c543b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readmitted\n",
      "NO     164592\n",
      ">30    106635\n",
      "<30     34071\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(merged_df[\"readmitted\"].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1bc32d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_readmit_multi\n",
      "no_readmit     164592\n",
      "readmit_>30    106635\n",
      "readmit_<30     34071\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "merged_df[\"target_readmit_30\"] = merged_df[\"readmitted\"].apply(\n",
    "    lambda x: 1 if str(x).strip() == \"<30\" else 0\n",
    ")\n",
    "merged_df[\"target_readmit_over30\"] = merged_df[\"readmitted\"].apply(\n",
    "    lambda x: 1 if str(x).strip() == \">30\" else 0\n",
    ")\n",
    "def classify_readmit(x):\n",
    "    x = str(x).strip()\n",
    "    if x == \"<30\":\n",
    "        return \"readmit_<30\"\n",
    "    elif x == \">30\":\n",
    "        return \"readmit_>30\"\n",
    "    elif x == \"NO\":\n",
    "        return \"no_readmit\"\n",
    "    else:\n",
    "        return \"unknown\"\n",
    "\n",
    "merged_df[\"target_readmit_multi\"] = merged_df[\"readmitted\"].apply(classify_readmit)\n",
    "print(merged_df[\"target_readmit_multi\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7587124f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>readmitted</th>\n",
       "      <th>target_readmit_30</th>\n",
       "      <th>target_readmit_over30</th>\n",
       "      <th>target_readmit_multi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>no_readmit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>no_readmit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>no_readmit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&gt;30</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>readmit_&gt;30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&gt;30</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>readmit_&gt;30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&gt;30</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>readmit_&gt;30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>no_readmit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>no_readmit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>no_readmit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>no_readmit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  readmitted  target_readmit_30  target_readmit_over30 target_readmit_multi\n",
       "0         NO                  0                      0           no_readmit\n",
       "1         NO                  0                      0           no_readmit\n",
       "2         NO                  0                      0           no_readmit\n",
       "3        >30                  0                      1          readmit_>30\n",
       "4        >30                  0                      1          readmit_>30\n",
       "5        >30                  0                      1          readmit_>30\n",
       "6         NO                  0                      0           no_readmit\n",
       "7         NO                  0                      0           no_readmit\n",
       "8         NO                  0                      0           no_readmit\n",
       "9         NO                  0                      0           no_readmit"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df[[\"readmitted\", \"target_readmit_30\", \"target_readmit_over30\", \"target_readmit_multi\"]].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "34026a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_over30 = merged_df[\"target_readmit_over30\"]\n",
    "y_multi = merged_df[\"target_readmit_multi\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ce32df13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (244238, 15), Test: (61060, 15)\n",
      "Classes: ['no_readmit', 'readmit_<30', 'readmit_>30']\n",
      "Training multinomial Logistic Regression …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sk151\\OneDrive\\Desktop\\assesment2\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sk151\\OneDrive\\Desktop\\assesment2\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Training complete.\n",
      "\n",
      "Accuracy       : 0.488\n",
      "F1 (macro)     : 0.401\n",
      "F1 (weighted)  : 0.485\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  no_readmit      0.634     0.675     0.654     32919\n",
      " readmit_<30      0.184     0.390     0.250      6814\n",
      " readmit_>30      0.424     0.230     0.299     21327\n",
      "\n",
      "    accuracy                          0.488     61060\n",
      "   macro avg      0.414     0.432     0.401     61060\n",
      "weighted avg      0.510     0.488     0.485     61060\n",
      "\n",
      "Confusion matrix:\n",
      " [[22209  5402  5308]\n",
      " [ 2801  2658  1355]\n",
      " [10029  6386  4912]]\n",
      "\n",
      "Macro ROC-AUC (OvR): 0.633\n",
      "Total model features after preprocessing: 27\n"
     ]
    }
   ],
   "source": [
    "# ==== Multiclass readmission model: no_readmit vs readmit_>30 vs readmit_<30 ====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, label_binarize\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, classification_report, confusion_matrix,\n",
    "    roc_auc_score\n",
    ")\n",
    "\n",
    "# ---- 0) Target & features ----\n",
    "TARGET = \"target_readmit_multi\"  # values: 'no_readmit', 'readmit_>30', 'readmit_<30'\n",
    "assert TARGET in merged_df.columns, f\"{TARGET} not found in merged_df\"\n",
    "\n",
    "# X = everything except any target/readmitted columns\n",
    "drop_cols = [c for c in merged_df.columns if c.startswith(\"target_readmit\")] + [\"readmitted\"]\n",
    "X = merged_df.drop(columns=drop_cols, errors=\"ignore\")\n",
    "y = merged_df[TARGET].astype(str)\n",
    "\n",
    "# ---- 1) Preprocessing ----\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = [c for c in X.columns if c not in num_cols]\n",
    "\n",
    "numeric_pipe = Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\"))])\n",
    "categorical_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n",
    "])\n",
    "\n",
    "pre = ColumnTransformer([\n",
    "    (\"num\", numeric_pipe, num_cols),\n",
    "    (\"cat\", categorical_pipe, cat_cols),\n",
    "])\n",
    "\n",
    "# ---- 2) Multinomial Logistic Regression (fast & balanced) ----\n",
    "clf = Pipeline([\n",
    "    (\"pre\", pre),\n",
    "    (\"clf\", LogisticRegression(\n",
    "        solver=\"saga\",\n",
    "        multi_class=\"multinomial\",\n",
    "        class_weight=\"balanced\",\n",
    "        max_iter=500,\n",
    "        n_jobs=-1,\n",
    "        tol=1e-3\n",
    "    ))\n",
    "])\n",
    "\n",
    "# ---- 3) Train / test split ----\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "print(f\"Classes: {sorted(y.unique().tolist())}\")\n",
    "\n",
    "# ---- 4) Fit ----\n",
    "print(\"Training multinomial Logistic Regression …\")\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"✓ Training complete.\\n\")\n",
    "\n",
    "# ---- 5) Evaluate ----\n",
    "pred = clf.predict(X_test)\n",
    "proba = clf.predict_proba(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, pred)\n",
    "f1_macro = f1_score(y_test, pred, average=\"macro\")\n",
    "f1_weighted = f1_score(y_test, pred, average=\"weighted\")\n",
    "\n",
    "print(\"Accuracy       :\", round(acc, 3))\n",
    "print(\"F1 (macro)     :\", round(f1_macro, 3))\n",
    "print(\"F1 (weighted)  :\", round(f1_weighted, 3))\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_test, pred, digits=3))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, pred, labels=sorted(y.unique())))\n",
    "\n",
    "# ---- 6) Macro ROC-AUC (One-vs-Rest) ----\n",
    "classes = sorted(y.unique())\n",
    "y_test_bin = label_binarize(y_test, classes=classes)\n",
    "auc_macro_ovr = roc_auc_score(y_test_bin, proba, average=\"macro\", multi_class=\"ovr\")\n",
    "print(\"\\nMacro ROC-AUC (OvR):\", round(auc_macro_ovr, 3))\n",
    "\n",
    "# ---- 7) (Optional) Inspect total feature count after OHE ----\n",
    "ohe = clf.named_steps[\"pre\"].named_transformers_.get(\"cat\")\n",
    "if ohe is not None:\n",
    "    ohe = ohe.named_steps[\"ohe\"]\n",
    "    total_features = len(num_cols) + len(ohe.get_feature_names_out())\n",
    "    print(\"Total model features after preprocessing:\", total_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "da4e588d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model saved successfully as readmission_multiclass.pkl\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the entire preprocessing + model pipeline\n",
    "joblib.dump(clf, \"readmission_multiclass.pkl\")\n",
    "\n",
    "print(\"✅ Model saved successfully as readmission_multiclass.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
